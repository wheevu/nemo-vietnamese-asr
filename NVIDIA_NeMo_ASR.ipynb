{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-So-hYuHPeZ"
      },
      "outputs": [],
      "source": [
        "# Step 1:\n",
        "# Mount your Google Drive.\n",
        "# You will be prompted to authorize access.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your project's base path in Google Drive for cleanliness.\n",
        "# IMPORTANT: Make sure you have created this folder in your Drive and uploaded your .zip file there.\n",
        "# For example, create a folder called \"Colab Notebooks\" and inside it, a folder called \"nemo_asr_project\".\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Colab Notebooks/nemo_asr_project\"\n",
        "\n",
        "# Step 2:\n",
        "# Install NVIDIA NeMo.\n",
        "!pip install nemo_toolkit['all']\n",
        "\n",
        "# Unzip your prepared dataset from Drive into the local Colab session for faster access during training.\n",
        "# We unzip it to /content/ for speed, but our source is safely in Drive.\n",
        "!unzip \"{PROJECT_PATH}/vn_asr_data_v1.zip\" -d /content/data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load the Pre-trained Model"
      ],
      "metadata": {
        "id": "jXYtglCcFHxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbfOjyAXHQ-t",
        "outputId": "ec0e5885-60a8-4b10-e0ae-57b456618abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "WARNING:megatron.core.utils:fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.\n",
            "WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
            "WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.\n",
            "[NeMo W 2025-12-25 15:28:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "    \n",
            "[NeMo W 2025-12-25 15:28:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "      m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "    \n",
            "[NeMo W 2025-12-25 15:28:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "    \n",
            "[NeMo W 2025-12-25 15:28:27 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "      elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load model 'stt_en_conformer_ctc_large' using the Universal Loader...\n",
            "[NeMo I 2025-12-25 15:28:28 nemo_logging:393] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_conformer_ctc_large/versions/1.10.0/files/stt_en_conformer_ctc_large.nemo to /root/.cache/torch/NeMo/NeMo_2.6.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
            "[NeMo I 2025-12-25 15:28:52 nemo_logging:393] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2025-12-25 15:28:54 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-12-25 15:28:54 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath:\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: true\n",
            "    num_workers: 4\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 20.0\n",
            "    min_duration: 0.1\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths:\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size:\n",
            "    - 34\n",
            "    - 30\n",
            "    - 26\n",
            "    - 22\n",
            "    - 18\n",
            "    - 16\n",
            "    - 12\n",
            "    - 8\n",
            "    \n",
            "[NeMo W 2025-12-25 15:28:54 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /manifests/librispeech/librivox-dev-other.json\n",
            "    - /manifests/librispeech/librivox-dev-clean.json\n",
            "    - /manifests/librispeech/librivox-test-other.json\n",
            "    - /manifests/librispeech/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n",
            "[NeMo W 2025-12-25 15:28:54 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /manifests/librispeech/librivox-dev-other.json\n",
            "    - /manifests/librispeech/librivox-dev-clean.json\n",
            "    - /manifests/librispeech/librivox-test-other.json\n",
            "    - /manifests/librispeech/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-12-25 15:28:54 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-12-25 15:28:56 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.6.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import os\n",
        "\n",
        "# --- Step 3: Load the Pre-trained Model ---\n",
        "\n",
        "# Keep the model name in a variable so later cells can reference it safely.\n",
        "model_name_ngc = \"stt_en_conformer_ctc_large\"\n",
        "\n",
        "# We use the generic 'ASRModel' class. This is the \"Universal Loader.\"\n",
        "# It automatically detects if the model needs BPE (like Conformer) or not.\n",
        "print(f\"Attempting to load model '{model_name_ngc}' using the Universal Loader...\")\n",
        "\n",
        "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_name_ngc)\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Verify Data Pipeline"
      ],
      "metadata": {
        "id": "_vPJs8KoFBbA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t1No1AaHhb4",
        "outputId": "a1631b1b-36ad-4366-e70e-333e10992156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading manifest from: /content/data/train_manifest.json\n",
            "Successfully loaded 7 samples from the training set.\n",
            "\n",
            "=== Data Pipeline Check ===\n",
            "Audio File: /Users/nguyenhuyvu/nemo-vietnamese-asr/audio/qZuxop5xj_E.wav\n",
            "Original Duration: 1287.48 seconds\n",
            "File Exists: False\n",
            "===========================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "manifest_path = '/content/data/train_manifest.json'\n",
        "\n",
        "print(f\"Reading manifest from: {manifest_path}\")\n",
        "\n",
        "# Read the manifest file\n",
        "data_entries = []\n",
        "with open(manifest_path, 'r') as f:\n",
        "    for line in f:\n",
        "        data_entries.append(json.loads(line))\n",
        "\n",
        "print(f\"Successfully loaded {len(data_entries)} samples from the training set.\")\n",
        "\n",
        "# Pick a random sample to inspect\n",
        "sample = data_entries[0] # Let's look at the first one\n",
        "audio_path = sample['audio_filepath']\n",
        "duration = sample['duration']\n",
        "\n",
        "print(\"\\n=== Data Pipeline Check ===\")\n",
        "print(f\"Audio File: {audio_path}\")\n",
        "print(f\"Original Duration: {duration:.2f} seconds\")\n",
        "print(f\"File Exists: {os.path.exists(audio_path)}\")\n",
        "print(\"===========================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: End-to-End Inference Test (Robust API Check)"
      ],
      "metadata": {
        "id": "kYyZWN1eE-gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzZRYR0sPzl-",
        "outputId": "4deb1fb9-886c-44a3-9e85-e5247bb8ff84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-12-13 10:30:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:30:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing audio for inference...\n",
            "Created temporary 30s inference clip: /content/temp_30s.wav\n",
            "Running NVIDIA NeMo Transcribe...\n",
            "\n",
            "--- Debug: Method Signature ---\n",
            "(audio: Union[str, List[str], torch.Tensor, numpy.ndarray, torch.utils.data.dataloader.DataLoader], batch_size: int = 4, return_hypotheses: bool = False, num_workers: int = 0, channel_selector: Union[int, Iterable[int], str, NoneType] = None, augmentor: omegaconf.dictconfig.DictConfig = None, verbose: bool = True, timestamps: Optional[bool] = None, override_config: Optional[nemo.collections.asr.parts.mixins.transcription.TranscribeConfig] = None) -> Union[List[str], List[nemo.collections.asr.parts.utils.rnnt_utils.Hypothesis], Tuple[List[str]], Tuple[List[nemo.collections.asr.parts.utils.rnnt_utils.Hypothesis]]]\n",
            "-------------------------------\n",
            "\n",
            "Attempting with 'paths2audio_files'...\n",
            "Attempting with positional arguments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:01,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "ðŸ¤– MODEL: stt_en_conformer_ctc_large\n",
            "ðŸ“‚ INPUT: qZuxop5xj_E.wav (First 30s)\n",
            "----------------------------------------\n",
            "ðŸŽ™ï¸ TRANSCRIPTION:\n",
            "'Hypothesis(score=tensor(-112.7178), y_sequence=tensor([128, 128, 128, 128,  66, 128, 128, 128,   8, 128, 128,  44, 128, 128,\n",
            "          8, 128,   3, 128, 128, 128, 128, 128, 128, 115, 128, 128, 128, 128,\n",
            "        128,  25, 128, 128,  47,   8,  18,  30, 128, 128,  31, 128, 128, 128,\n",
            "        128,  64, 128,  45, 128,  20, 128,   1,  30,  15,   4, 128, 128, 128,\n",
            "         50,   1, 128,  28, 128,  22, 128,  20,  12, 128, 128,   1, 128,  93,\n",
            "        128,   5, 128,   9,   6, 128, 128, 128, 128, 128, 128, 128,  63, 128,\n",
            "        128,   6, 128, 128,   1, 128, 128, 128, 128, 128, 106, 128,  28, 128,\n",
            "         31, 128, 128, 128, 128, 128,  85, 128, 128,   3, 128,  56, 128, 128,\n",
            "          3,  35, 128,   4,  42, 128,   1,  14,  28, 128,  22,   1,  11, 128,\n",
            "        128,  14, 128,  67, 128,   4,  47, 128, 128, 128, 128,  23, 128, 128,\n",
            "        128,   5, 128,   8, 128, 128,   1, 128, 128, 128,   1,  13,  12, 128,\n",
            "         60, 128,  35, 128, 128,  63, 128, 128,  12, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128,   1, 128,  14, 128,  28, 128,  22, 128,  11, 128,   9,\n",
            "        128,  14, 128, 128,  67, 128,   4, 128, 128,  97, 128, 128, 128,   6,\n",
            "        128, 128,   1, 128, 128, 128,   6, 128, 128,  23, 128, 128,   6,  22,\n",
            "        128,  31, 128, 128, 128, 128,   1, 128,  15, 128, 128,  66, 128,   4,\n",
            "         49, 128, 128,  71, 128, 128, 128, 128, 128, 128,  45, 128, 128, 128,\n",
            "        128, 128,  21, 128,   4, 128, 128,   4, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 113, 128, 128, 128, 128,  13,\n",
            "        128,  23, 128, 128, 128, 128, 128, 128, 128,  25, 128,  90, 128, 128,\n",
            "        128,  21, 128,   8,  13, 128,   7, 128, 128, 128, 128, 128, 107, 128,\n",
            "          2, 128,   3, 128,  21,  24, 128,   9, 128,   6,  89, 128,  41, 128,\n",
            "        128, 128,  13,   4, 128,  47, 128, 128, 128, 128,  23, 128, 128,  13,\n",
            "        128, 128,  71, 128, 128,  38, 128, 128,  12, 128, 115, 128, 128,   4,\n",
            "        128, 128, 128,  80,   4,   3, 128,  74,  25, 128,  91, 128, 128, 128,\n",
            "          5,   1,  13,  12,   1,  86,  86,  13,  44,  23, 128, 128, 128,  13,\n",
            "         43, 128, 128, 128, 115, 128, 128, 128,  66,  59, 128, 128, 128, 102,\n",
            "        128, 128, 128,   1,  19, 128,  13,  22,   1, 128,  86,  13, 128,  44,\n",
            "        102, 128, 128, 128,  36, 128,  11, 128,   3, 128,  90, 128, 128,   2,\n",
            "        128, 128, 128,  15, 128,  12, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "         23, 128, 128, 128, 128, 128, 128,   3, 128,  80,   8, 128,  13,   4,\n",
            "          1, 128, 128, 128, 128, 128, 128,  67, 128, 128,  47, 128,   8, 128,\n",
            "          3, 128, 113, 128, 128, 128, 128,  16, 128,  47, 128, 128, 113, 128,\n",
            "        128, 128,  16, 128,  23, 128, 128,   6,  22, 128,  47, 128,  12, 128,\n",
            "         23, 128, 128, 128, 128, 128,   3,   1,  19, 128,  65,  22, 128, 128,\n",
            "        128, 106, 128,  63, 128, 128,   6, 128, 128, 128, 128,  97, 128, 128,\n",
            "        128, 128, 128,  71, 128, 128, 128, 128,  77,  29, 128, 128, 128,  23,\n",
            "        128, 128,   6, 128, 128, 128, 128, 128, 128, 128,   1,   8, 128, 128,\n",
            "         46, 128, 128, 128,   6, 128, 107, 128, 128, 128,  25, 128, 128, 128,\n",
            "        128, 128, 128, 115, 128,   4, 128, 128, 128, 128, 128, 128, 128,  71,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128]), text='tabat coming back that on your knees angry radio no  mean that mat between lang ulone b toda  my ow ny languilone soo o toog that n tell do you see hom toing ca sam the most serious fme b tom doory come get thing did my lamb tomin com t he go hmg lamb go put casny tot game on bat hop b hop toog by tot hung me no so do was i too a wo moing come do', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)'\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "print(\"Preparing audio for inference...\")\n",
        "\n",
        "# 1. GET DATA FROM MANIFEST\n",
        "# We reuse the 'sample' loaded in Step 4\n",
        "original_path = sample['audio_filepath']\n",
        "filename = os.path.basename(original_path) # Extracts 'qZuxop5xj_E.wav'\n",
        "\n",
        "# 2. CONSTRUCT CORRECT COLAB PATH\n",
        "colab_audio_path = os.path.join('/content/data/audio', filename)\n",
        "\n",
        "if not os.path.exists(colab_audio_path):\n",
        "    raise FileNotFoundError(f\"Could not find audio at: {colab_audio_path}\")\n",
        "\n",
        "# 3. LOAD & CROP (First 30s)\n",
        "y, sr = librosa.load(colab_audio_path, sr=16000, duration=30.0)\n",
        "temp_path = '/content/temp_30s.wav'\n",
        "sf.write(temp_path, y, sr)\n",
        "print(f\"Created temporary 30s inference clip: {temp_path}\")\n",
        "\n",
        "# 4. RUN INFERENCE (With API Auto-Fix)\n",
        "print(\"Running NVIDIA NeMo Transcribe...\")\n",
        "asr_model.eval()\n",
        "\n",
        "# Let's inspect what the model expects (for your own knowledge)\n",
        "print(\"\\n--- Debug: Method Signature ---\")\n",
        "try:\n",
        "    import inspect\n",
        "    print(inspect.signature(asr_model.transcribe))\n",
        "except:\n",
        "    pass\n",
        "print(\"-------------------------------\\n\")\n",
        "\n",
        "transcriptions = []\n",
        "\n",
        "try:\n",
        "    # Attempt 1: The Standard Way (Keyword Argument)\n",
        "    print(\"Attempting with 'paths2audio_files'...\")\n",
        "    transcriptions = asr_model.transcribe(paths2audio_files=[temp_path])\n",
        "except TypeError:\n",
        "    try:\n",
        "        # Attempt 2: Positional Argument (Likely Fix)\n",
        "        print(\"Attempting with positional arguments...\")\n",
        "        transcriptions = asr_model.transcribe([temp_path])\n",
        "    except TypeError:\n",
        "        # Attempt 3: New 'audio' keyword (Possible 2.0 change)\n",
        "        print(\"Attempting with 'audio' keyword...\")\n",
        "        transcriptions = asr_model.transcribe(audio=[temp_path])\n",
        "\n",
        "# 5. DISPLAY RESULTS\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"ðŸ¤– MODEL: {model_name_ngc}\")\n",
        "print(f\"ðŸ“‚ INPUT: {filename} (First 30s)\")\n",
        "print(\"-\" * 40)\n",
        "print(\"ðŸŽ™ï¸ TRANSCRIPTION:\")\n",
        "if transcriptions:\n",
        "    print(f\"'{transcriptions[0]}'\")\n",
        "else:\n",
        "    print(\"ERROR: Could not run inference. Check logs.\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Step 6: Manual Evaluation Loop"
      ],
      "metadata": {
        "id": "whPOQar0E4m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfgce6KhIujM",
        "outputId": "3abd6fd0-9266-4a3f-a063-d8e4720a399e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-12-13 10:35:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:35:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Manual Evaluation on Test Set...\n",
            "Found 2 samples in test set.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:00,  5.20it/s]\n",
            "[NeMo W 2025-12-13 10:35:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:35:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "Ref: chÃ o báº¡n cáº£m Æ¡n báº¡n Ä‘Ã£ nháº¥n nghe giang ngÆ¡i radio ...\n",
            "Hyp: ta bak coming back do onion n ang the radio no mei...\n",
            "WER: 1.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:00,  7.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 2 ---\n",
            "Ref: chÃ o báº¡n cáº£m Æ¡n báº¡n Ä‘Ã£ nháº¥n nghe ra hÆ¡i radio nÆ¡i ...\n",
            "Hyp: tbak coming back don your nas on the radio no momi...\n",
            "WER: 1.00\n",
            "\n",
            "========================================\n",
            "Evaluation Complete. Average WER: 1.00\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import jiwer\n",
        "\n",
        "\n",
        "print(\"Starting Manual Evaluation on Test Set...\")\n",
        "\n",
        "test_manifest_path = '/content/data/test_manifest.json'\n",
        "\n",
        "# 1. Read the Test Manifest\n",
        "with open(test_manifest_path, 'r') as f:\n",
        "    test_samples = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Found {len(test_samples)} samples in test set.\")\n",
        "\n",
        "# 2. Evaluation Loop\n",
        "asr_model.eval()\n",
        "\n",
        "total_wer = 0\n",
        "count = 0\n",
        "\n",
        "for sample in test_samples:\n",
        "    original_path = sample['audio_filepath']\n",
        "    reference_text = sample['text']\n",
        "    filename = os.path.basename(original_path)\n",
        "\n",
        "    colab_path = os.path.join('/content/data/audio', filename)\n",
        "\n",
        "    if not os.path.exists(colab_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Crop to 30s\n",
        "        y, sr = librosa.load(colab_path, sr=16000, duration=30.0)\n",
        "        temp_path = f\"/content/temp_eval_{count}.wav\"\n",
        "        sf.write(temp_path, y, sr)\n",
        "\n",
        "        # Transcribe\n",
        "        try:\n",
        "            # Try keyword arg first\n",
        "            preds = asr_model.transcribe(paths2audio_files=[temp_path])\n",
        "        except TypeError:\n",
        "            # Fallback to positional\n",
        "            preds = asr_model.transcribe([temp_path])\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # Check if output is a string or an object\n",
        "        hypothesis = preds[0]\n",
        "        if not isinstance(hypothesis, str):\n",
        "            # It's a Hypothesis object, extract the text\n",
        "            hypothesis = hypothesis.text\n",
        "\n",
        "        # Calculate Metrics\n",
        "        wer = jiwer.wer(reference_text, hypothesis)\n",
        "        total_wer += wer\n",
        "        count += 1\n",
        "\n",
        "        print(f\"\\n--- Sample {count} ---\")\n",
        "        print(f\"Ref: {reference_text[:50]}...\")\n",
        "        print(f\"Hyp: {hypothesis[:50]}...\")\n",
        "        print(f\"WER: {wer:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "if count > 0:\n",
        "    avg_wer = total_wer / count\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"Evaluation Complete. Average WER: {avg_wer:.2f}\")\n",
        "    print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Save your final model directly to your Google Drive for persistence."
      ],
      "metadata": {
        "id": "kJgsVdY7E16W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9nxMuetIwK5",
        "outputId": "731e3625-fb38-4358-851b-f7771b5c4330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the final model to: /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo\n",
            "Model saved successfully to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "model_save_path = os.path.join(PROJECT_PATH, \"vietnamese_asr_v1.nemo\")\n",
        "print(f\"Saving the final model to: {model_save_path}\")\n",
        "asr_model.save_to(model_save_path)\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Quantization Benchmark (float32 vs float16 vs int8)\n",
        "\n",
        "This section benchmarks inference latency and WER across different precision settings on Colab T4.\n",
        "\n",
        "It runs the repoâ€™s `benchmark.py`, prints a Markdown table, and you can paste the output into the READMEâ€™s â€œProduction-Ready Model Optimizationâ€ section."
      ],
      "metadata": {
        "id": "Io8h5sJbExSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8A: Fetch repo code (so benchmark.py exists) + install benchmark dependencies\n",
        "\n",
        "# This notebook is intended to be uploaded and run in Colab.\n",
        "# To avoid any mismatch between your local changes and the GitHub repo state, this cell will:\n",
        "#  1) Clone the repo (for folder structure)\n",
        "#  2) Install required packages\n",
        "#  3) Write stable benchmark + helper modules into the cloned repo\n",
        "#  4) Sanity-check that they compile\n",
        "\n",
        "REPO_URL = \"https://github.com/wheevu/nemo-vietnamese-asr.git\"\n",
        "REPO_DIR = \"/content/nemo-vietnamese-asr\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "\n",
        "%cd {REPO_DIR}\n",
        "\n",
        "# Ensure dependencies are present (Colab usually includes torch already).\n",
        "# NeMo is installed in Step 1, but keep a safety install in case you run only Step 8.\n",
        "!pip -q install \"nemo_toolkit[all]\"\n",
        "!pip -q install -r requirements.txt\n",
        "!pip -q install jiwer \"quanto==0.0.11\" librosa\n",
        "\n",
        "# Write helper modules used by benchmark.py\n",
        "Path(\"src\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "(Path(\"src\") / \"__init__.py\").write_text(r'''\"\"\"Workspace-local utilities.\"\"\"\n",
        "''', encoding=\"utf-8\")\n",
        "\n",
        "(Path(\"src\") / \"model_utils.py\").write_text(r'''from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def get_default_device() -> torch.device:\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def load_nemo_model(model_ref: str, device: Optional[torch.device] = None):\n",
        "    device = device or get_default_device()\n",
        "    import nemo.collections.asr as nemo_asr\n",
        "\n",
        "    if model_ref.endswith(\".nemo\") and os.path.exists(model_ref):\n",
        "        model = nemo_asr.models.ASRModel.restore_from(restore_path=model_ref)\n",
        "    else:\n",
        "        model = nemo_asr.models.ASRModel.from_pretrained(model_name=model_ref)\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    return model\n",
        "\n",
        "\n",
        "def _quanto_quantize_in_place(model) -> None:\n",
        "    from quanto import freeze, quantize\n",
        "    model.eval()\n",
        "    try:\n",
        "        from quanto import qint8\n",
        "        try:\n",
        "            quantize(model, weights=qint8)\n",
        "        except TypeError:\n",
        "            quantize(model, qint8)\n",
        "    except Exception:\n",
        "        quantize(model)\n",
        "    freeze(model)\n",
        "\n",
        "\n",
        "def _quanto_quantize_linear_layers_in_place(model) -> int:\n",
        "    import torch.nn as nn\n",
        "    from quanto import freeze, quantize\n",
        "    try:\n",
        "        from quanto import qint8\n",
        "        q = qint8\n",
        "    except Exception:\n",
        "        q = None\n",
        "\n",
        "    quantized_count = 0\n",
        "    for _name, module in model.named_modules():\n",
        "        if not isinstance(module, nn.Linear):\n",
        "            continue\n",
        "        try:\n",
        "            if q is None:\n",
        "                quantize(module)\n",
        "            else:\n",
        "                try:\n",
        "                    quantize(module, weights=q)\n",
        "                except TypeError:\n",
        "                    quantize(module, q)\n",
        "            quantized_count += 1\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if quantized_count > 0:\n",
        "        freeze(model)\n",
        "    return quantized_count\n",
        "\n",
        "\n",
        "def apply_quantization(model, precision: str):\n",
        "    precision = (precision or \"float32\").lower().strip()\n",
        "\n",
        "    if precision == \"float32\":\n",
        "        return model\n",
        "\n",
        "    if precision == \"float16\":\n",
        "        return model.half()\n",
        "\n",
        "    if precision == \"int8\":\n",
        "        try:\n",
        "            _quanto_quantize_in_place(model)\n",
        "            return model\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Full-model INT8 failed: {e}. Trying Linear-only.\", RuntimeWarning)\n",
        "\n",
        "        try:\n",
        "            n = _quanto_quantize_linear_layers_in_place(model)\n",
        "            if n > 0:\n",
        "                warnings.warn(f\"Linear-only INT8 succeeded for {n} layers.\", RuntimeWarning)\n",
        "                return model\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Linear-only INT8 failed: {e}.\", RuntimeWarning)\n",
        "\n",
        "        warnings.warn(\"Falling back to float32.\", RuntimeWarning)\n",
        "        return model\n",
        "\n",
        "    raise ValueError(f\"Unsupported precision: {precision}\")\n",
        "''', encoding=\"utf-8\")\n",
        "\n",
        "(Path(\"src\") / \"inference.py\").write_text(r'''from __future__ import annotations\n",
        "\n",
        "import inspect\n",
        "from typing import List, Sequence\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def transcribe_audio(model, audio_files: Sequence[str], batch_size: int = 1) -> List[str]:\n",
        "    if not audio_files:\n",
        "        return []\n",
        "\n",
        "    def _to_text(p) -> str:\n",
        "        if hasattr(p, \"text\"):\n",
        "            try:\n",
        "                return str(getattr(p, \"text\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return str(p)\n",
        "\n",
        "    model.eval()\n",
        "    # Use no_grad instead of inference_mode for Quanto compatibility\n",
        "    with torch.no_grad():\n",
        "        transcribe_fn = getattr(model, \"transcribe\")\n",
        "\n",
        "        sig = None\n",
        "        try:\n",
        "            sig = inspect.signature(transcribe_fn)\n",
        "        except (TypeError, ValueError):\n",
        "            sig = None\n",
        "\n",
        "        if sig is not None and \"paths2audio_files\" in sig.parameters:\n",
        "            preds = transcribe_fn(paths2audio_files=list(audio_files), batch_size=batch_size)\n",
        "        elif sig is not None and \"audio\" in sig.parameters:\n",
        "            preds = transcribe_fn(audio=list(audio_files), batch_size=batch_size)\n",
        "        else:\n",
        "            try:\n",
        "                preds = transcribe_fn(list(audio_files), batch_size=batch_size)\n",
        "            except TypeError:\n",
        "                preds = transcribe_fn(list(audio_files))\n",
        "\n",
        "    return [_to_text(p) for p in preds]\n",
        "''', encoding=\"utf-8\")\n",
        "\n",
        "# Write benchmark.py with 30s chunking + graceful int8 error handling\n",
        "Path(\"benchmark.py\").write_text(r'''from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import hashlib\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import tempfile\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from src.inference import transcribe_audio\n",
        "from src.model_utils import apply_quantization, get_default_device, load_nemo_model\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ManifestSample:\n",
        "    audio_filepath: str\n",
        "    text: str\n",
        "\n",
        "\n",
        "def read_manifest_samples(manifest_path: str, limit: int) -> List[ManifestSample]:\n",
        "    samples: List[ManifestSample] = []\n",
        "    with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            if \"audio_filepath\" not in obj or \"text\" not in obj:\n",
        "                continue\n",
        "            samples.append(\n",
        "                ManifestSample(audio_filepath=str(obj[\"audio_filepath\"]), text=str(obj[\"text\"]))\n",
        "            )\n",
        "            if len(samples) >= limit:\n",
        "                break\n",
        "    return samples\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    return \" \".join((text or \"\").strip().lower().split())\n",
        "\n",
        "\n",
        "def compute_wer_percent(references: Sequence[str], hypotheses: Sequence[str]) -> float:\n",
        "    import jiwer\n",
        "    refs = [normalize_text(r) for r in references]\n",
        "    hyps = [normalize_text(h) for h in hypotheses]\n",
        "    return float(jiwer.wer(refs, hyps) * 100.0)\n",
        "\n",
        "\n",
        "def serialized_state_dict_size_mb(model) -> float:\n",
        "    buf = io.BytesIO()\n",
        "    cpu_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
        "    torch.save(cpu_state, buf)\n",
        "    return buf.getbuffer().nbytes / (1024 * 1024)\n",
        "\n",
        "\n",
        "def cuda_sync_if_needed(device: torch.device) -> None:\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "\n",
        "def maybe_remap_audio_path(path: str, audio_root: Optional[str]) -> str:\n",
        "    if os.path.exists(path) or not audio_root:\n",
        "        return path\n",
        "    return os.path.join(audio_root, os.path.basename(path))\n",
        "\n",
        "\n",
        "MAX_AUDIO_DURATION_S = 30.0\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "\n",
        "\n",
        "def prepare_audio_for_benchmark(path: str) -> str:\n",
        "    \"\"\"Prepare audio: mono, 16kHz, max 30s to avoid Conformer OOM.\"\"\"\n",
        "    import librosa\n",
        "    import numpy as np\n",
        "    import soundfile as sf\n",
        "\n",
        "    tmp_dir = os.path.join(tempfile.gettempdir(), \"nemo_vietnamese_asr_bench\")\n",
        "    os.makedirs(tmp_dir, exist_ok=True)\n",
        "\n",
        "    info = sf.info(path)\n",
        "    key_src = f\"{os.path.abspath(path)}|{info.samplerate}|{info.channels}|{info.duration}|{MAX_AUDIO_DURATION_S}\".encode(\"utf-8\")\n",
        "    key = hashlib.sha1(key_src).hexdigest()[:12]\n",
        "    base = os.path.splitext(os.path.basename(path))[0]\n",
        "    out_path = os.path.join(tmp_dir, f\"{base}.prep.{key}.wav\")\n",
        "    if os.path.exists(out_path):\n",
        "        return out_path\n",
        "\n",
        "    y, sr = librosa.load(path, sr=TARGET_SAMPLE_RATE, mono=True, duration=MAX_AUDIO_DURATION_S)\n",
        "    sf.write(out_path, y, sr)\n",
        "    return out_path\n",
        "\n",
        "\n",
        "def collect_existing_pairs(\n",
        "    samples: Sequence[ManifestSample], audio_root: Optional[str]\n",
        ") -> Tuple[List[str], List[str]]:\n",
        "    audio_paths: List[str] = []\n",
        "    references: List[str] = []\n",
        "\n",
        "    for s in samples:\n",
        "        p = maybe_remap_audio_path(s.audio_filepath, audio_root)\n",
        "        if not os.path.exists(p):\n",
        "            continue\n",
        "        try:\n",
        "            p = prepare_audio_for_benchmark(p)\n",
        "        except Exception:\n",
        "            continue\n",
        "        audio_paths.append(p)\n",
        "        references.append(s.text)\n",
        "\n",
        "    return audio_paths, references\n",
        "\n",
        "\n",
        "def benchmark_one_precision(\n",
        "    *,\n",
        "    model_ref: str,\n",
        "    manifest_path: str,\n",
        "    precision: str,\n",
        "    sample_limit: int,\n",
        "    batch_size: int,\n",
        "    warmup_runs: int,\n",
        "    device: torch.device,\n",
        "    audio_root: Optional[str],\n",
        ") -> Dict[str, object]:\n",
        "    # Reload model each run (no state leakage)\n",
        "    try:\n",
        "        model = load_nemo_model(model_ref, device=device)\n",
        "        model = apply_quantization(model, precision)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"Precision\": precision,\n",
        "            \"Avg Latency (ms/file)\": None,\n",
        "            \"WER (%)\": None,\n",
        "            \"VRAM/Size (MB)\": None,\n",
        "            \"Samples\": 0,\n",
        "            \"Notes\": f\"Load/quantization failed: {type(e).__name__}\",\n",
        "        }\n",
        "\n",
        "    samples = read_manifest_samples(manifest_path, limit=sample_limit)\n",
        "    audio_files, references = collect_existing_pairs(samples, audio_root)\n",
        "\n",
        "    model_size_mb = serialized_state_dict_size_mb(model)\n",
        "\n",
        "    if not audio_files:\n",
        "        return {\n",
        "            \"Precision\": precision,\n",
        "            \"Avg Latency (ms/file)\": None,\n",
        "            \"WER (%)\": None,\n",
        "            \"VRAM/Size (MB)\": round(model_size_mb, 2),\n",
        "            \"Samples\": 0,\n",
        "            \"Notes\": \"No audio files found.\",\n",
        "        }\n",
        "\n",
        "    warmup_batch = audio_files[: max(1, min(batch_size, len(audio_files)))]\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max(0, warmup_runs)):\n",
        "                _ = transcribe_audio(model, warmup_batch, batch_size=len(warmup_batch))\n",
        "    except RuntimeError as e:\n",
        "        return {\n",
        "            \"Precision\": precision,\n",
        "            \"Avg Latency (ms/file)\": None,\n",
        "            \"WER (%)\": None,\n",
        "            \"VRAM/Size (MB)\": round(model_size_mb, 2),\n",
        "            \"Samples\": 0,\n",
        "            \"Notes\": f\"Inference failed (Quanto issue): {type(e).__name__}\",\n",
        "        }\n",
        "\n",
        "    vram_peak_mb: Optional[float] = None\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    hypotheses: List[str] = []\n",
        "    cuda_sync_if_needed(device)\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(audio_files), batch_size):\n",
        "                batch = audio_files[i : i + batch_size]\n",
        "                hypotheses.extend(transcribe_audio(model, batch, batch_size=len(batch)))\n",
        "    except RuntimeError as e:\n",
        "        return {\n",
        "            \"Precision\": precision,\n",
        "            \"Avg Latency (ms/file)\": None,\n",
        "            \"WER (%)\": None,\n",
        "            \"VRAM/Size (MB)\": round(model_size_mb, 2),\n",
        "            \"Samples\": 0,\n",
        "            \"Notes\": f\"Inference failed: {type(e).__name__}\",\n",
        "        }\n",
        "\n",
        "    cuda_sync_if_needed(device)\n",
        "    t1 = time.perf_counter()\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        vram_peak_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
        "\n",
        "    total_s = t1 - t0\n",
        "    avg_latency_ms = (total_s / len(audio_files)) * 1000.0\n",
        "    wer_percent = compute_wer_percent(references, hypotheses)\n",
        "\n",
        "    vram_or_size_mb = vram_peak_mb if vram_peak_mb is not None else model_size_mb\n",
        "\n",
        "    return {\n",
        "        \"Precision\": precision,\n",
        "        \"Avg Latency (ms/file)\": round(avg_latency_ms, 2),\n",
        "        \"WER (%)\": round(wer_percent, 2),\n",
        "        \"VRAM/Size (MB)\": round(vram_or_size_mb, 2),\n",
        "        \"Samples\": len(audio_files),\n",
        "    }\n",
        "\n",
        "\n",
        "def results_to_markdown_table(rows: List[Dict[str, object]]) -> str:\n",
        "    cols = [\"Precision\", \"Avg Latency (ms/file)\", \"WER (%)\", \"VRAM/Size (MB)\", \"Samples\", \"Notes\"]\n",
        "    header = \"| \" + \" | \".join(cols) + \" |\"\n",
        "    sep = \"| \" + \" | \".join([\"---\"] * len(cols)) + \" |\"\n",
        "    body = [\"| \" + \" | \".join(str(r.get(c, \"\")) for c in cols) + \" |\" for r in rows]\n",
        "    return \"\\n\".join([header, sep] + body)\n",
        "\n",
        "\n",
        "def main() -> int:\n",
        "    parser = argparse.ArgumentParser(description=\"Benchmark NeMo ASR quantization.\")\n",
        "    parser.add_argument(\"--model\", required=True)\n",
        "    parser.add_argument(\"--manifest\", default=\"val_manifest.json\")\n",
        "    parser.add_argument(\"--samples\", type=int, default=100)\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=1)\n",
        "    parser.add_argument(\"--warmup-runs\", dest=\"warmup_runs\", type=int, default=3)\n",
        "    parser.add_argument(\"--warmup-batches\", dest=\"warmup_runs\", type=int)\n",
        "    parser.add_argument(\"--audio-root\", default=None)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    device = get_default_device()\n",
        "    precisions = [\"float32\", \"float16\", \"int8\"]\n",
        "\n",
        "    results: List[Dict[str, object]] = []\n",
        "    for p in precisions:\n",
        "        print(f\"\\n>>> Benchmarking {p}...\")\n",
        "        results.append(\n",
        "            benchmark_one_precision(\n",
        "                model_ref=args.model,\n",
        "                manifest_path=args.manifest,\n",
        "                precision=p,\n",
        "                sample_limit=max(1, args.samples),\n",
        "                batch_size=max(1, args.batch_size),\n",
        "                warmup_runs=max(0, args.warmup_runs or 0),\n",
        "                device=device,\n",
        "                audio_root=args.audio_root,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(\"\\n=== Benchmark Results (raw) ===\")\n",
        "    for r in results:\n",
        "        print(r)\n",
        "\n",
        "    print(\"\\n=== Markdown Table (paste into README) ===\")\n",
        "    print(results_to_markdown_table(results))\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raise SystemExit(main())\n",
        "''', encoding=\"utf-8\")\n",
        "\n",
        "# Sanity check: compile the generated files\n",
        "!python -m py_compile benchmark.py src/model_utils.py src/inference.py\n",
        "\n",
        "print(\"Repo ready for benchmarking:\", os.getcwd())\n",
        "print(\"Wrote updated benchmark + helper modules.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VZVqFXTEzDw",
        "outputId": "a8924d53-3320-42f6-b074-b148deed45a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nemo-vietnamese-asr\n",
            "Repo ready for benchmarking: /content/nemo-vietnamese-asr\n",
            "Wrote updated benchmark + helper modules.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8B: Run benchmark.py (prints a Markdown table for README)\n",
        "\n",
        "# Choose which model to benchmark:\n",
        "import os\n",
        "\n",
        "# Prefer your trained NeMo artifact if it exists; otherwise fall back to the NGC model name.\n",
        "DEFAULT_NGC = globals().get(\"model_name_ngc\", \"stt_en_conformer_ctc_large\")\n",
        "SAVED_NEMO = os.path.join(PROJECT_PATH, \"vietnamese_asr_v1.nemo\")\n",
        "MODEL_REF = SAVED_NEMO if os.path.exists(SAVED_NEMO) else DEFAULT_NGC\n",
        "print(\"Benchmarking model:\", MODEL_REF)\n",
        "\n",
        "# Use whichever manifest you want to evaluate on; start with test to keep it small.\n",
        "MANIFEST_PATH = \"/content/data/test_manifest.json\"\n",
        "\n",
        "# Remap macOS absolute paths by basename into Colab audio dir (matches your notebook's path-bridging approach).\n",
        "AUDIO_ROOT = \"/content/data/audio\"\n",
        "\n",
        "# Keep runtime reasonable on Colab\n",
        "SAMPLES = 100\n",
        "BATCH_SIZE = 1\n",
        "WARMUP_RUNS = 3\n",
        "\n",
        "!python benchmark.py --model \"{MODEL_REF}\" --manifest \"{MANIFEST_PATH}\" --samples {SAMPLES} --batch-size {BATCH_SIZE} --warmup-runs {WARMUP_RUNS} --audio-root \"{AUDIO_ROOT}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZiQnwAvFh4S",
        "outputId": "ea69396b-ea72-439f-f65a-6c244b374e13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking model: /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "\n",
            ">>> Benchmarking float32...\n",
            "fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.\n",
            "2025-12-25 16:03:54.480746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1766678634.501492   13257 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1766678634.507728   13257 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1766678634.524798   13257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678634.524834   13257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678634.524840   13257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1766678634.524844   13257 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
            "No exporters were provided. This means that no telemetry data will be collected.\n",
            "[NeMo I 2025-12-25 16:04:06 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n",
            "[NeMo W 2025-12-25 16:04:06 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /content/data/train_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: true\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 3600\n",
            "    min_duration: 0.0\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size:\n",
            "    - 34\n",
            "    - 30\n",
            "    - 26\n",
            "    - 22\n",
            "    - 18\n",
            "    - 16\n",
            "    - 12\n",
            "    - 8\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:06 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /content/data/val_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: false\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    max_duration: 3600\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:06 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: /content/data/test_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n",
            "[NeMo I 2025-12-25 16:04:06 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-12-25 16:04:08 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo.\n",
            "[NeMo W 2025-12-25 16:04:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  1.94it/s]\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  6.58it/s]\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  7.98it/s]\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  8.00it/s]\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  7.96it/s]\n",
            "\n",
            ">>> Benchmarking float16...\n",
            "[NeMo I 2025-12-25 16:04:11 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n",
            "[NeMo W 2025-12-25 16:04:11 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /content/data/train_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: true\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 3600\n",
            "    min_duration: 0.0\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size:\n",
            "    - 34\n",
            "    - 30\n",
            "    - 26\n",
            "    - 22\n",
            "    - 18\n",
            "    - 16\n",
            "    - 12\n",
            "    - 8\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:11 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /content/data/val_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: false\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    max_duration: 3600\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:11 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: /content/data/test_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n",
            "[NeMo I 2025-12-25 16:04:11 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-12-25 16:04:13 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo.\n",
            "[NeMo W 2025-12-25 16:04:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00,  2.52it/s]\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00, 11.00it/s]\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00, 14.30it/s]\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00, 14.73it/s]\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 1it [00:00, 16.36it/s]\n",
            "\n",
            ">>> Benchmarking int8...\n",
            "[NeMo I 2025-12-25 16:04:16 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n",
            "[NeMo W 2025-12-25 16:04:16 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath: /content/data/train_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: true\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 3600\n",
            "    min_duration: 0.0\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size:\n",
            "    - 34\n",
            "    - 30\n",
            "    - 26\n",
            "    - 22\n",
            "    - 18\n",
            "    - 16\n",
            "    - 12\n",
            "    - 8\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:16 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath: /content/data/val_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: false\n",
            "    num_workers: 2\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    max_duration: 3600\n",
            "    \n",
            "[NeMo W 2025-12-25 16:04:16 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: /content/data/test_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n",
            "[NeMo I 2025-12-25 16:04:16 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-12-25 16:04:18 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo.\n",
            "[NeMo W 2025-12-25 16:04:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-25 16:04:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
            "Transcribing: 0it [00:00, ?it/s]\n",
            "\n",
            "=== Benchmark Results (raw) ===\n",
            "{'Precision': 'float32', 'Avg Latency (ms/file)': 151.54, 'WER (%)': 99.89, 'VRAM/Size (MB)': 731.29, 'Samples': 2}\n",
            "{'Precision': 'float16', 'Avg Latency (ms/file)': 89.34, 'WER (%)': 99.88, 'VRAM/Size (MB)': 888.88, 'Samples': 2}\n",
            "{'Precision': 'int8', 'Avg Latency (ms/file)': None, 'WER (%)': None, 'VRAM/Size (MB)': 166.05, 'Samples': 0, 'Notes': 'Inference failed (Quanto issue): RuntimeError'}\n",
            "\n",
            "=== Markdown Table (paste into README) ===\n",
            "| Precision | Avg Latency (ms/file) | WER (%) | VRAM/Size (MB) | Samples | Notes |\n",
            "| --- | --- | --- | --- | --- | --- |\n",
            "| float32 | 151.54 | 99.89 | 731.29 | 2 |  |\n",
            "| float16 | 89.34 | 99.88 | 888.88 | 2 |  |\n",
            "| int8 | None | None | 166.05 | 0 | Inference failed (Quanto issue): RuntimeError |\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}