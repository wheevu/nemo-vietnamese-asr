{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-So-hYuHPeZ",
        "outputId": "67a3cc81-451b-40c9-cafb-ca14e5fd39f2"
      },
      "outputs": [],
      "source": [
        "# Step 1: Mount your Google Drive.\n",
        "# You will be prompted to authorize access.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define your project's base path in Google Drive for cleanliness.\n",
        "# IMPORTANT: Make sure you have created this folder in your Drive and uploaded your .zip file there.\n",
        "# For example, create a folder called \"Colab Notebooks\" and inside it, a folder called \"nemo_asr_project\".\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/Colab Notebooks/nemo_asr_project\"\n",
        "\n",
        "# Step 3: Install NVIDIA NeMo.\n",
        "!pip install nemo_toolkit['all']\n",
        "\n",
        "# Step 4: Unzip your prepared dataset from Drive into the local Colab session for faster access during training.\n",
        "# We unzip it to /content/ for speed, but our source is safely in Drive.\n",
        "!unzip \"{PROJECT_PATH}/vn_asr_data_v1.zip\" -d /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbfOjyAXHQ-t",
        "outputId": "f474cdeb-df1a-40b4-c40f-b3b30db25ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to load model 'stt_en_conformer_ctc_large' using the Universal Loader...\n",
            "[NeMo I 2025-12-13 10:16:31 nemo_logging:393] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_conformer_ctc_large/versions/1.10.0/files/stt_en_conformer_ctc_large.nemo to /root/.cache/torch/NeMo/NeMo_2.6.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo\n",
            "[NeMo I 2025-12-13 10:16:33 nemo_logging:393] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2025-12-13 10:16:38 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-12-13 10:16:38 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath:\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/tarred_audio_manifest.json\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/tarred_audio_manifest.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 1\n",
            "    shuffle: true\n",
            "    num_workers: 4\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    trim_silence: false\n",
            "    max_duration: 20.0\n",
            "    min_duration: 0.1\n",
            "    is_tarred: true\n",
            "    tarred_audio_filepaths:\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket1/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket2/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket3/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket4/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket5/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket6/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket7/audio__OP_0..8191_CL_.tar\n",
            "    - - /data2/nemo_asr/nemo_asr_set_3.0/bucket8/audio__OP_0..8191_CL_.tar\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size:\n",
            "    - 34\n",
            "    - 30\n",
            "    - 26\n",
            "    - 22\n",
            "    - 18\n",
            "    - 16\n",
            "    - 12\n",
            "    - 8\n",
            "    \n",
            "[NeMo W 2025-12-13 10:16:38 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /manifests/librispeech/librivox-dev-other.json\n",
            "    - /manifests/librispeech/librivox-dev-clean.json\n",
            "    - /manifests/librispeech/librivox-test-other.json\n",
            "    - /manifests/librispeech/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n",
            "[NeMo W 2025-12-13 10:16:38 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath:\n",
            "    - /manifests/librispeech/librivox-dev-other.json\n",
            "    - /manifests/librispeech/librivox-dev-clean.json\n",
            "    - /manifests/librispeech/librivox-test-other.json\n",
            "    - /manifests/librispeech/librivox-test-clean.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 32\n",
            "    shuffle: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    use_start_end_token: false\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2025-12-13 10:16:38 nemo_logging:393] PADDING: 0\n",
            "[NeMo I 2025-12-13 10:16:40 nemo_logging:393] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_2.6.0/stt_en_conformer_ctc_large/afb212c5bcf904e326b5e5751e7c7465/stt_en_conformer_ctc_large.nemo.\n",
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import nemo.collections.asr as nemo_asr\n",
        "import os\n",
        "\n",
        "# --- Step 3: Load the Pre-trained Model ---\n",
        "\n",
        "# We use the generic 'ASRModel' class. This is the \"Universal Loader.\"\n",
        "# It automatically detects if the model needs BPE (like Conformer) or not.\n",
        "print(f\"Attempting to load model 'stt_en_conformer_ctc_large' using the Universal Loader...\")\n",
        "\n",
        "asr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"stt_en_conformer_ctc_large\")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t1No1AaHhb4",
        "outputId": "a1631b1b-36ad-4366-e70e-333e10992156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading manifest from: /content/data/train_manifest.json\n",
            "Successfully loaded 7 samples from the training set.\n",
            "\n",
            "=== Data Pipeline Check ===\n",
            "Audio File: /Users/nguyenhuyvu/nemo-vietnamese-asr/audio/qZuxop5xj_E.wav\n",
            "Original Duration: 1287.48 seconds\n",
            "File Exists: False\n",
            "===========================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Step 4: Verify Data Pipeline ---\n",
        "\n",
        "manifest_path = '/content/data/train_manifest.json'\n",
        "\n",
        "print(f\"Reading manifest from: {manifest_path}\")\n",
        "\n",
        "# Read the manifest file\n",
        "data_entries = []\n",
        "with open(manifest_path, 'r') as f:\n",
        "    for line in f:\n",
        "        data_entries.append(json.loads(line))\n",
        "\n",
        "print(f\"Successfully loaded {len(data_entries)} samples from the training set.\")\n",
        "\n",
        "# Pick a random sample to inspect\n",
        "sample = data_entries[0] # Let's look at the first one\n",
        "audio_path = sample['audio_filepath']\n",
        "duration = sample['duration']\n",
        "\n",
        "print(\"\\n=== Data Pipeline Check ===\")\n",
        "print(f\"Audio File: {audio_path}\")\n",
        "print(f\"Original Duration: {duration:.2f} seconds\")\n",
        "print(f\"File Exists: {os.path.exists(audio_path)}\")\n",
        "print(\"===========================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzZRYR0sPzl-",
        "outputId": "4deb1fb9-886c-44a3-9e85-e5247bb8ff84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-12-13 10:30:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:30:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing audio for inference...\n",
            "Created temporary 30s inference clip: /content/temp_30s.wav\n",
            "Running NVIDIA NeMo Transcribe...\n",
            "\n",
            "--- Debug: Method Signature ---\n",
            "(audio: Union[str, List[str], torch.Tensor, numpy.ndarray, torch.utils.data.dataloader.DataLoader], batch_size: int = 4, return_hypotheses: bool = False, num_workers: int = 0, channel_selector: Union[int, Iterable[int], str, NoneType] = None, augmentor: omegaconf.dictconfig.DictConfig = None, verbose: bool = True, timestamps: Optional[bool] = None, override_config: Optional[nemo.collections.asr.parts.mixins.transcription.TranscribeConfig] = None) -> Union[List[str], List[nemo.collections.asr.parts.utils.rnnt_utils.Hypothesis], Tuple[List[str]], Tuple[List[nemo.collections.asr.parts.utils.rnnt_utils.Hypothesis]]]\n",
            "-------------------------------\n",
            "\n",
            "Attempting with 'paths2audio_files'...\n",
            "Attempting with positional arguments...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:01,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "ðŸ¤– MODEL: stt_en_conformer_ctc_large\n",
            "ðŸ“‚ INPUT: qZuxop5xj_E.wav (First 30s)\n",
            "----------------------------------------\n",
            "ðŸŽ™ï¸ TRANSCRIPTION:\n",
            "'Hypothesis(score=tensor(-112.7178), y_sequence=tensor([128, 128, 128, 128,  66, 128, 128, 128,   8, 128, 128,  44, 128, 128,\n",
            "          8, 128,   3, 128, 128, 128, 128, 128, 128, 115, 128, 128, 128, 128,\n",
            "        128,  25, 128, 128,  47,   8,  18,  30, 128, 128,  31, 128, 128, 128,\n",
            "        128,  64, 128,  45, 128,  20, 128,   1,  30,  15,   4, 128, 128, 128,\n",
            "         50,   1, 128,  28, 128,  22, 128,  20,  12, 128, 128,   1, 128,  93,\n",
            "        128,   5, 128,   9,   6, 128, 128, 128, 128, 128, 128, 128,  63, 128,\n",
            "        128,   6, 128, 128,   1, 128, 128, 128, 128, 128, 106, 128,  28, 128,\n",
            "         31, 128, 128, 128, 128, 128,  85, 128, 128,   3, 128,  56, 128, 128,\n",
            "          3,  35, 128,   4,  42, 128,   1,  14,  28, 128,  22,   1,  11, 128,\n",
            "        128,  14, 128,  67, 128,   4,  47, 128, 128, 128, 128,  23, 128, 128,\n",
            "        128,   5, 128,   8, 128, 128,   1, 128, 128, 128,   1,  13,  12, 128,\n",
            "         60, 128,  35, 128, 128,  63, 128, 128,  12, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128,   1, 128,  14, 128,  28, 128,  22, 128,  11, 128,   9,\n",
            "        128,  14, 128, 128,  67, 128,   4, 128, 128,  97, 128, 128, 128,   6,\n",
            "        128, 128,   1, 128, 128, 128,   6, 128, 128,  23, 128, 128,   6,  22,\n",
            "        128,  31, 128, 128, 128, 128,   1, 128,  15, 128, 128,  66, 128,   4,\n",
            "         49, 128, 128,  71, 128, 128, 128, 128, 128, 128,  45, 128, 128, 128,\n",
            "        128, 128,  21, 128,   4, 128, 128,   4, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128, 113, 128, 128, 128, 128,  13,\n",
            "        128,  23, 128, 128, 128, 128, 128, 128, 128,  25, 128,  90, 128, 128,\n",
            "        128,  21, 128,   8,  13, 128,   7, 128, 128, 128, 128, 128, 107, 128,\n",
            "          2, 128,   3, 128,  21,  24, 128,   9, 128,   6,  89, 128,  41, 128,\n",
            "        128, 128,  13,   4, 128,  47, 128, 128, 128, 128,  23, 128, 128,  13,\n",
            "        128, 128,  71, 128, 128,  38, 128, 128,  12, 128, 115, 128, 128,   4,\n",
            "        128, 128, 128,  80,   4,   3, 128,  74,  25, 128,  91, 128, 128, 128,\n",
            "          5,   1,  13,  12,   1,  86,  86,  13,  44,  23, 128, 128, 128,  13,\n",
            "         43, 128, 128, 128, 115, 128, 128, 128,  66,  59, 128, 128, 128, 102,\n",
            "        128, 128, 128,   1,  19, 128,  13,  22,   1, 128,  86,  13, 128,  44,\n",
            "        102, 128, 128, 128,  36, 128,  11, 128,   3, 128,  90, 128, 128,   2,\n",
            "        128, 128, 128,  15, 128,  12, 128, 128, 128, 128, 128, 128, 128, 128,\n",
            "         23, 128, 128, 128, 128, 128, 128,   3, 128,  80,   8, 128,  13,   4,\n",
            "          1, 128, 128, 128, 128, 128, 128,  67, 128, 128,  47, 128,   8, 128,\n",
            "          3, 128, 113, 128, 128, 128, 128,  16, 128,  47, 128, 128, 113, 128,\n",
            "        128, 128,  16, 128,  23, 128, 128,   6,  22, 128,  47, 128,  12, 128,\n",
            "         23, 128, 128, 128, 128, 128,   3,   1,  19, 128,  65,  22, 128, 128,\n",
            "        128, 106, 128,  63, 128, 128,   6, 128, 128, 128, 128,  97, 128, 128,\n",
            "        128, 128, 128,  71, 128, 128, 128, 128,  77,  29, 128, 128, 128,  23,\n",
            "        128, 128,   6, 128, 128, 128, 128, 128, 128, 128,   1,   8, 128, 128,\n",
            "         46, 128, 128, 128,   6, 128, 107, 128, 128, 128,  25, 128, 128, 128,\n",
            "        128, 128, 128, 115, 128,   4, 128, 128, 128, 128, 128, 128, 128,  71,\n",
            "        128, 128, 128, 128, 128, 128, 128, 128]), text='tabat coming back that on your knees angry radio no  mean that mat between lang ulone b toda  my ow ny languilone soo o toog that n tell do you see hom toing ca sam the most serious fme b tom doory come get thing did my lamb tomin com t he go hmg lamb go put casny tot game on bat hop b hop toog by tot hung me no so do was i too a wo moing come do', dec_out=None, dec_state=None, timestamp=[], alignments=None, frame_confidence=None, token_confidence=None, word_confidence=None, length=0, y=None, lm_state=None, lm_scores=None, ngram_lm_state=None, tokens=None, last_token=None, token_duration=None, last_frame=None)'\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "import json\n",
        "\n",
        "# --- Step 5: End-to-End Inference Test (Robust API Check) ---\n",
        "\n",
        "print(\"Preparing audio for inference...\")\n",
        "\n",
        "# 1. GET DATA FROM MANIFEST\n",
        "# We reuse the 'sample' loaded in Step 4\n",
        "original_path = sample['audio_filepath']\n",
        "filename = os.path.basename(original_path) # Extracts 'qZuxop5xj_E.wav'\n",
        "\n",
        "# 2. CONSTRUCT CORRECT COLAB PATH\n",
        "colab_audio_path = os.path.join('/content/data/audio', filename)\n",
        "\n",
        "if not os.path.exists(colab_audio_path):\n",
        "    raise FileNotFoundError(f\"Could not find audio at: {colab_audio_path}\")\n",
        "\n",
        "# 3. LOAD & CROP (First 30s)\n",
        "y, sr = librosa.load(colab_audio_path, sr=16000, duration=30.0)\n",
        "temp_path = '/content/temp_30s.wav'\n",
        "sf.write(temp_path, y, sr)\n",
        "print(f\"Created temporary 30s inference clip: {temp_path}\")\n",
        "\n",
        "# 4. RUN INFERENCE (With API Auto-Fix)\n",
        "print(\"Running NVIDIA NeMo Transcribe...\")\n",
        "asr_model.eval()\n",
        "\n",
        "# Let's inspect what the model expects (for your own knowledge)\n",
        "print(\"\\n--- Debug: Method Signature ---\")\n",
        "try:\n",
        "    import inspect\n",
        "    print(inspect.signature(asr_model.transcribe))\n",
        "except:\n",
        "    pass\n",
        "print(\"-------------------------------\\n\")\n",
        "\n",
        "transcriptions = []\n",
        "\n",
        "try:\n",
        "    # Attempt 1: The Standard Way (Keyword Argument)\n",
        "    print(\"Attempting with 'paths2audio_files'...\")\n",
        "    transcriptions = asr_model.transcribe(paths2audio_files=[temp_path])\n",
        "except TypeError:\n",
        "    try:\n",
        "        # Attempt 2: Positional Argument (Likely Fix)\n",
        "        print(\"Attempting with positional arguments...\")\n",
        "        transcriptions = asr_model.transcribe([temp_path])\n",
        "    except TypeError:\n",
        "        # Attempt 3: New 'audio' keyword (Possible 2.0 change)\n",
        "        print(\"Attempting with 'audio' keyword...\")\n",
        "        transcriptions = asr_model.transcribe(audio=[temp_path])\n",
        "\n",
        "# 5. DISPLAY RESULTS\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"ðŸ¤– MODEL: {model_name_ngc}\")\n",
        "print(f\"ðŸ“‚ INPUT: {filename} (First 30s)\")\n",
        "print(\"-\" * 40)\n",
        "print(\"ðŸŽ™ï¸ TRANSCRIPTION:\")\n",
        "if transcriptions:\n",
        "    print(f\"'{transcriptions[0]}'\")\n",
        "else:\n",
        "    print(\"ERROR: Could not run inference. Check logs.\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfgce6KhIujM",
        "outputId": "3abd6fd0-9266-4a3f-a063-d8e4720a399e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2025-12-13 10:35:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:35:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Manual Evaluation on Test Set...\n",
            "Found 2 samples in test set.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:00,  5.20it/s]\n",
            "[NeMo W 2025-12-13 10:35:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
            "[NeMo W 2025-12-13 10:35:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "Ref: chÃ o báº¡n cáº£m Æ¡n báº¡n Ä‘Ã£ nháº¥n nghe giang ngÆ¡i radio ...\n",
            "Hyp: ta bak coming back do onion n ang the radio no mei...\n",
            "WER: 1.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Transcribing: 1it [00:00,  7.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 2 ---\n",
            "Ref: chÃ o báº¡n cáº£m Æ¡n báº¡n Ä‘Ã£ nháº¥n nghe ra hÆ¡i radio nÆ¡i ...\n",
            "Hyp: tbak coming back don your nas on the radio no momi...\n",
            "WER: 1.00\n",
            "\n",
            "========================================\n",
            "Evaluation Complete. Average WER: 1.00\n",
            "========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import jiwer\n",
        "\n",
        "# --- Step 6: Manual Evaluation Loop (Final Fix) ---\n",
        "\n",
        "print(\"Starting Manual Evaluation on Test Set...\")\n",
        "\n",
        "test_manifest_path = '/content/data/test_manifest.json'\n",
        "\n",
        "# 1. Read the Test Manifest\n",
        "with open(test_manifest_path, 'r') as f:\n",
        "    test_samples = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"Found {len(test_samples)} samples in test set.\")\n",
        "\n",
        "# 2. Evaluation Loop\n",
        "asr_model.eval()\n",
        "\n",
        "total_wer = 0\n",
        "count = 0\n",
        "\n",
        "for sample in test_samples:\n",
        "    original_path = sample['audio_filepath']\n",
        "    reference_text = sample['text']\n",
        "    filename = os.path.basename(original_path)\n",
        "\n",
        "    colab_path = os.path.join('/content/data/audio', filename)\n",
        "\n",
        "    if not os.path.exists(colab_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Crop to 30s\n",
        "        y, sr = librosa.load(colab_path, sr=16000, duration=30.0)\n",
        "        temp_path = f\"/content/temp_eval_{count}.wav\"\n",
        "        sf.write(temp_path, y, sr)\n",
        "\n",
        "        # Transcribe\n",
        "        try:\n",
        "            # Try keyword arg first\n",
        "            preds = asr_model.transcribe(paths2audio_files=[temp_path])\n",
        "        except TypeError:\n",
        "            # Fallback to positional\n",
        "            preds = asr_model.transcribe([temp_path])\n",
        "\n",
        "        # --- THE FIX IS HERE ---\n",
        "        # Check if output is a string or an object\n",
        "        hypothesis = preds[0]\n",
        "        if not isinstance(hypothesis, str):\n",
        "            # It's a Hypothesis object, extract the text\n",
        "            hypothesis = hypothesis.text\n",
        "\n",
        "        # Calculate Metrics\n",
        "        wer = jiwer.wer(reference_text, hypothesis)\n",
        "        total_wer += wer\n",
        "        count += 1\n",
        "\n",
        "        print(f\"\\n--- Sample {count} ---\")\n",
        "        print(f\"Ref: {reference_text[:50]}...\")\n",
        "        print(f\"Hyp: {hypothesis[:50]}...\")\n",
        "        print(f\"WER: {wer:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "if count > 0:\n",
        "    avg_wer = total_wer / count\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"Evaluation Complete. Average WER: {avg_wer:.2f}\")\n",
        "    print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9nxMuetIwK5",
        "outputId": "731e3625-fb38-4358-851b-f7771b5c4330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving the final model to: /content/drive/MyDrive/Colab Notebooks/nemo_asr_project/vietnamese_asr_v1.nemo\n",
            "Model saved successfully to Google Drive!\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Save your final model directly to your Google Drive for persistence.\n",
        "import os\n",
        "\n",
        "model_save_path = os.path.join(PROJECT_PATH, \"vietnamese_asr_v1.nemo\")\n",
        "print(f\"Saving the final model to: {model_save_path}\")\n",
        "asr_model.save_to(model_save_path)\n",
        "print(\"Model saved successfully to Google Drive!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
